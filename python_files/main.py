# -*- coding: utf-8 -*-
"""Braining_Mini_Challenge_LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hnDZI1812-Qwgrp4iLUZZe1iCKNbUV_C

## Predicting Stroke Risk Based on Lifestyle Habits - Logistic Regression Model

#### Import libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.utils import resample
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, StratifiedKFold

"""#### Read data"""

stroke_data = pd.read_csv("/content/I05-0006 stroke_risk.csv")
stroke_data.head()

"""#### Exploratory Data Analysis"""

stroke_data.info()

stroke_data.describe()

stroke_data.isnull().sum()

"""### Data Preprocessing

##### Outlier Detection and Winsorization
"""

def detect_outliers_iqr(df, column):
  """
  Detects outliers in a column
  df: dataframe
  column: column name
  returns: lower and upper bounds of the column (useful for winsorization)
  """
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR

  outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
  print(f"\nOutliers in {column}:")
  print(outliers[[column]])

  return lower_bound, upper_bound

# Previous visualization revealed outliers in these two classes, inspect them
bmi_lower, bmi_upper = detect_outliers_iqr(stroke_data, "bmi")
sleep_lower, sleep_upper = detect_outliers_iqr(stroke_data, "sleep_hours")

def cap_outliers(df, column, lower_bound, upper_bound):
  """
  Cap outliers in a column
  df: dataframe
  column: column name
  lower_bound: lower bound of the column
  upper_bound: upper bound of the column
  returns: dataframe with outliers capped
  """
  df[column] = df[column].clip(lower=lower_bound, upper=upper_bound)
  return df

stroke_data = cap_outliers(stroke_data, "bmi", bmi_lower, bmi_upper)
stroke_data = cap_outliers(stroke_data, "sleep_hours", sleep_lower, sleep_upper)

"""### Feature Engineering"""

# Bin ages into classes for clearer visualization of strok risk trend across age groups
bins = [30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80]
labels = ["30–34", "35-39", "40–44", "45-49", "50–54", "55-59", "60–64", "65-69", "70–74", "75-79"]
stroke_data["age_group"] = pd.cut(stroke_data["age"], bins=bins, labels=labels, right=False)
stroke_data.head()

# Plot of stroke risk distribution across age groups to visualize possible trends
age_stroke_counts = stroke_data.groupby(["age_group", "stroke_risk"], observed=True).size().reset_index(name="count")
plt.figure(figsize=(8,6))
sns.barplot(data=age_stroke_counts, x="age_group", y="count", hue="stroke_risk")

plt.title("Stroke Risk Distribution Across Age Groups")
plt.xlabel("Age Group")
plt.ylabel("Number of People")
plt.legend(title="Stroke Risk")
plt.show()

age_stroke_pivot = stroke_data.pivot_table(index="age_group", columns="stroke_risk",values="exercise_frequency",
                                          aggfunc='count',fill_value=0, observed=True)
age_stroke_pivot

# Visualization showing the relationship between exercise frequency, age group, and stroke risk (useful for making decisions about feature engineering)
g = sns.catplot(data=stroke_data, x="age_group", hue="stroke_risk", col="exercise_frequency",
            kind="count", height=4,aspect=1)
for ax in g.axes.flatten():
    ax.tick_params(axis='x', rotation=90)
plt.subplots_adjust(top=0.8)
plt.suptitle("Stroke Risk by Age Group and Exercise Frequency")
plt.show()

def get_unique_cat(data, col):
  """
  Get unique values in a column
  data: dataframe
  col: column name
  returns: unique values in the column
  """
  return data[col].unique()

# Print unique values in categorical columns
categorical_cols = ["smoking_status", "exercise_frequency", "stroke_risk"]
for i in categorical_cols:
  print(f"Unique values in {i}: {get_unique_cat(stroke_data, i)}")

# Ordinal encoding for categorical columns
cat_cols = ["exercise_frequency", "stroke_risk"]
exercise_frequency_map = {"High":0, "Low":2, "Moderate":1}
stroke_risk_map = {"No":0, "Yes":1}
maps = [exercise_frequency_map, stroke_risk_map]

def ordinal_encoding(data, columns, maps):
  """
  Ordinal encoding for categorical columns
  data: dataframe
  columns: list of column names
  maps: list of maps for each column
  returns: dataframe with ordinal encoded columns
  """
  data = data.copy()
  for col, mapping in zip(columns, maps):
    data[col] = data[col].map(mapping)
  return data

# Confirm ordinal encoding
stroke_df = ordinal_encoding(stroke_data, cat_cols, maps)
for i in cat_cols:
  print(f"Unique values in {i}: {get_unique_cat(stroke_df, i)}")

# One hot encoding for the smoking status column
stroke_df = pd.get_dummies(stroke_df, columns=["smoking_status"], drop_first=True, dtype=int)

# Adding new variables to account for feature relationships and importance
# 1 if age is less than 50, 2 if it is greater
stroke_df["age_above_50"] = (stroke_df["age"] > 50).astype(int) + 1
# product of exercise frequency and age
stroke_df["exercise_x_age"] = stroke_df["exercise_frequency"] * stroke_df["age"]
# product of exercise frequency and bmi
stroke_df["exercise_x_bmi"] = stroke_df["exercise_frequency"] * stroke_df["bmi"]
# variable to account for the relationship between bmi, exercise frequency and age class (product)
stroke_df["exercise_bmi_50"] = stroke_df["exercise_frequency"] * stroke_df["bmi"] * stroke_df["age_above_50"]
# product of sleep hours and exercise frequency
stroke_df["sleep_hours"] = stroke_df["sleep_hours"] * stroke_df["exercise_frequency"]
# Square the age variable to capture non-linearities
stroke_df["age_squared"] = stroke_df["age"] ** 2

def classify_bmi(bmi):
  """
  Classify bmi based on WHO metrics
  bmi: bmi value
  returns: bmi category
  """
  if bmi < 18.5:
      return 1
  elif 18.5 <= bmi < 25:
      return 2
  elif 25 <= bmi < 30:
      return 3
  elif 30 <= bmi < 35:
      return 4
  elif 35 <= bmi < 40:
      return 5
  else:
      return 6

stroke_df["bmi_class"] = stroke_df["bmi"].apply(classify_bmi)

# Inspect number of people in each bmi category
stroke_df["bmi_class"].value_counts()

# bmi values in class 5 are mostly for no-risk patients, confusing the model
# so drop values in that class
stroke_df = stroke_df[~stroke_df['bmi_class'].isin([5])].reset_index(drop=True)

# Add weight to BMI based on class
stroke_df["bmi"] = stroke_df["bmi"] * stroke_df["bmi_class"]
stroke_df["bmi_weighted"] = stroke_df["bmi"] * stroke_df["bmi_class"]

# Round sleep duration values to whole numbers
stroke_df["sleep_rounded"] = stroke_df["sleep_hours"].round()

# Inspect changes to dataframe
stroke_df.head()

"""#### Checking for class imbalance in the target variable"""

class_counts = stroke_df['stroke_risk'].value_counts()
print("Number of samples per class:")
print(class_counts)

# Set Matplotlib parameters for figure size
plt.rcParams['figure.figsize'] = (8, 6)

# Visualize target variable count
sns.countplot(x='stroke_risk', data=stroke_df, hue="stroke_risk", palette=["gray","red"])
plt.title("Stroke Risk Class Distribution")
plt.xlabel("Stroke Risk")
plt.ylabel("Frequency")
plt.show()

"""#### Check for collinearity"""

plt.figure(figsize=(12, 8))
corr = stroke_df.corr(numeric_only=True)
sns.heatmap(corr, annot=True, fmt=".2f", cmap="Reds", square=True, cbar=True)
plt.title("Correlation Heatmap of Stroke Dataset Variables")
plt.show()

# Drop one column for columns with high correlations
X = stroke_df.drop(["stroke_risk", "age_group", "age", "age_above_50", "bmi", "sleep_hours"], axis=1)
y = stroke_df["stroke_risk"]

"""#### Deal with class imbalance"""

# Smote to upsample the minority class
smote = SMOTE(sampling_strategy={1: 200}, random_state=42)
X_smote, y_smote = smote.fit_resample(X,y)

# Random undersampling to downsample the majority class
undersample = RandomUnderSampler(random_state=7)
X_under, y_under = undersample.fit_resample(X_smote, y_smote)

class_counts = y_under.value_counts()
print("Number of samples per class:")
print(class_counts)

# Visualize target variable count after handling imbalance
palette = {0: "grey", 1: "red"}
sns.countplot(x=y_under, palette=["gray","red"], hue=y_under)
plt.title("Class distribution after undersampling")
plt.xlabel("Target class")
plt.ylabel("Count")
plt.show()

"""### Model Training and Evaluation"""

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.2, random_state=42)

# Standardization for better results in logistic regression
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Fit logistic regressor
lr = LogisticRegression(C=1, penalty = "l1", solver = "saga", max_iter=5000)
lr.fit(X_train, y_train)

# Make predictions and obtain metrics
y_pred = lr.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))
print("Confusion Matrix:\n", confusion_mat)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Display confusion matrix
sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=["No","Yes"], yticklabels=["No","Yes"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

# Display ROC curve
y_prob = lr.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2,
         label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve\nAccuracy: {:.2f}%'.format(
    accuracy * 100))
plt.legend(loc="lower right")
plt.show()